{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eiurs1FFY3g"
      },
      "source": [
        "# Trabalho AV3 de Ciência de Dados\n",
        "#### Carolina de Souza Ribeiro (2110868)\n",
        "#### Marcelo de Souza Ribeiro (2417152)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldG9Bf6ei3C2"
      },
      "source": [
        "# Início"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k07Da_Si68z"
      },
      "source": [
        "## Import e configuração das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE13FmNiFWu1",
        "outputId": "6bdf24db-1ea1-43ac-e1dc-40458d3e16d0"
      },
      "outputs": [],
      "source": [
        "# Imports principais\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bibliotecas importadas com sucesso!\n",
            "Versão do pandas: 2.2.3\n",
            "Versão do numpy: 2.2.6\n"
          ]
        }
      ],
      "source": [
        "# Configurações de visualização\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_style(\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações do pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n",
        "print(f\"Versão do pandas: {pd.__version__}\")\n",
        "print(f\"Versão do numpy: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bibliotecas importadas com sucesso!\n",
            "Versão do pandas: 2.2.3\n",
            "Versão do numpy: 2.2.6\n"
          ]
        }
      ],
      "source": [
        "# Configurações de visualização\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_style(\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações do pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n",
        "print(f\"Versão do pandas: {pd.__version__}\")\n",
        "print(f\"Versão do numpy: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB8BXo1sizcf"
      },
      "source": [
        "## Download do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTnghYD7FkE5",
        "outputId": "74641186-fa95-4324-c00c-17285075632d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baixando dataset da Olist do Kaggle...\n",
            "Dataset baixado com sucesso!\n",
            "Caminho dos arquivos: C:\\Users\\carol\\.cache\\kagglehub\\datasets\\olistbr\\brazilian-ecommerce\\versions\\2\n",
            "\n",
            "9 arquivos CSV encontrados:\n",
            "  1. olist_customers_dataset.csv (8.62 MB)\n",
            "  2. olist_geolocation_dataset.csv (58.44 MB)\n",
            "  3. olist_orders_dataset.csv (16.84 MB)\n",
            "  4. olist_order_items_dataset.csv (14.72 MB)\n",
            "  5. olist_order_payments_dataset.csv (5.51 MB)\n",
            "  6. olist_order_reviews_dataset.csv (13.78 MB)\n",
            "  7. olist_products_dataset.csv (2.27 MB)\n",
            "  8. olist_sellers_dataset.csv (0.17 MB)\n",
            "  9. product_category_name_translation.csv (0.00 MB)\n"
          ]
        }
      ],
      "source": [
        "# Download do dataset diretamente do Kaggle\n",
        "print(\"Baixando dataset da Olist do Kaggle...\")\n",
        "\n",
        "try:\n",
        "    path = kagglehub.dataset_download(\"olistbr/brazilian-ecommerce\")\n",
        "    print(f\"Dataset baixado com sucesso!\")\n",
        "    print(f\"Caminho dos arquivos: {path}\")\n",
        "\n",
        "    # Listar todos os arquivos no diretório\n",
        "    files = os.listdir(path)\n",
        "    csv_files = [f for f in files if f.endswith(\".csv\")]\n",
        "\n",
        "    print(f\"\\n{len(csv_files)} arquivos CSV encontrados:\")\n",
        "    for i, file in enumerate(csv_files, 1):\n",
        "        file_path = os.path.join(path, file)\n",
        "        file_size = os.path.getsize(file_path) / (1024*1024)  # MB\n",
        "        print(f\"  {i}. {file} ({file_size:.2f} MB)\")\n",
        "\n",
        "    # Salvar o caminho para uso posterior\n",
        "    DATA_PATH = path\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Erro no download: {e}\")\n",
        "    print(\"Verifique se você tem acesso ao Kaggle e as credenciais configuradas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39wqlbZlGVWr",
        "outputId": "d245e843-9685-4c00-e2f1-336d8416adfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fazendo inspeção rápida de todos os arquivos...\n",
            "\n",
            "==================================================\n",
            "TABELA: olist_customers_dataset.csv\n",
            "==================================================\n",
            "Shape: (99441, 5)\n",
            "Colunas: ['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state']\n",
            "Memória: 29.62 MB\n",
            "\n",
            "==================================================\n",
            "TABELA: olist_geolocation_dataset.csv\n",
            "==================================================\n",
            "Shape: (1000163, 5)\n",
            "Colunas: ['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng', 'geolocation_city', 'geolocation_state']\n",
            "Memória: 145.20 MB\n",
            "\n",
            "==================================================\n",
            "TABELA: olist_orders_dataset.csv\n",
            "==================================================\n",
            "Shape: (99441, 8)\n",
            "Colunas: ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
            "Memória: 58.97 MB\n",
            "\n",
            "==================================================\n",
            "TABELA: olist_order_items_dataset.csv\n",
            "==================================================\n",
            "Shape: (112650, 7)\n",
            "Colunas: ['order_id', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', 'freight_value']\n",
            "Memória: 39.43 MB\n",
            "\n",
            "==================================================\n",
            "TABELA: olist_order_payments_dataset.csv\n",
            "==================================================\n",
            "Shape: (103886, 5)\n",
            "Colunas: ['order_id', 'payment_sequential', 'payment_type', 'payment_installments', 'payment_value']\n",
            "Memória: 17.81 MB\n",
            "\n",
            "==================================================\n",
            "TABELA: olist_order_reviews_dataset.csv\n",
            "==================================================\n",
            "Shape: (99224, 7)\n",
            "Colunas: ['review_id', 'order_id', 'review_score', 'review_comment_title', 'review_comment_message', 'review_creation_date', 'review_answer_timestamp']\n",
            "Memória: 42.75 MB\n",
            "\n",
            "==================================================\n",
            "TABELA: olist_products_dataset.csv\n",
            "==================================================\n",
            "Shape: (32951, 9)\n",
            "Colunas: ['product_id', 'product_category_name', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n",
            "Memória: 6.79 MB\n",
            "\n",
            "==================================================\n",
            "TABELA: olist_sellers_dataset.csv\n",
            "==================================================\n",
            "Shape: (3095, 4)\n",
            "Colunas: ['seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state']\n",
            "Memória: 0.66 MB\n",
            "\n",
            "==================================================\n",
            "TABELA: product_category_name_translation.csv\n",
            "==================================================\n",
            "Shape: (71, 2)\n",
            "Colunas: ['product_category_name', 'product_category_name_english']\n",
            "Memória: 0.01 MB\n",
            "\n",
            "Inspeção inicial completa!\n",
            "Total de registros no dataset: 1,550,922\n"
          ]
        }
      ],
      "source": [
        "# Função auxiliar para carregar e inspecionar cada tabela\n",
        "def quick_inspect(filename, data_path):\n",
        "    \"\"\"Carrega um CSV e faz inspeção básica\"\"\"\n",
        "    filepath = os.path.join(data_path, filename)\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"TABELA: {filename}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Colunas: {list(df.columns)}\")\n",
        "    print(f\"Memória: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Verificar se temos o caminho dos dados\n",
        "if 'DATA_PATH' in locals():\n",
        "    print(\"Fazendo inspeção rápida de todos os arquivos...\")\n",
        "\n",
        "    # Dicionário para armazenar os DataFrames\n",
        "    dfs = {}\n",
        "\n",
        "    # Inspecionar cada arquivo CSV\n",
        "    for csv_file in csv_files:\n",
        "        table_name = csv_file.replace('.csv', '').replace('olist_', '')\n",
        "        dfs[table_name] = quick_inspect(csv_file, DATA_PATH)\n",
        "\n",
        "    print(f\"\\nInspeção inicial completa!\")\n",
        "    print(f\"Total de registros no dataset: {sum(df.shape[0] for df in dfs.values()):,}\")\n",
        "\n",
        "else:\n",
        "    print(\"Erro: DATA_PATH não foi definido. Execute a célula anterior primeiro.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHv66C8NjLek"
      },
      "source": [
        "## Análises para definição de problemas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-neDk2YHub2"
      },
      "source": [
        "### Análise Exploratória das Tabelas Principais\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XYoscu-jfGE"
      },
      "source": [
        "Agora que temos uma visão geral do dataset, vamos analisar em detalhes as tabelas que serão centrais para nossos problemas de machine learning. Começaremos pela tabela de pedidos (orders), que funciona como a espinha dorsal do dataset, e depois examinaremos as tabelas relacionadas.\n",
        "\n",
        "O objetivo desta etapa é entender:\n",
        "- A estrutura e qualidade dos dados em cada tabela\n",
        "- Distribuições das variáveis principais\n",
        "- Presença de valores ausentes ou inconsistências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp58-pVZHvoA",
        "outputId": "e5191024-37b0-44cf-bd58-2f756b75c273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANÁLISE DA TABELA ORDERS\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Análise detalhada da tabela principal de pedidos\n",
        "print(\"ANÁLISE DA TABELA ORDERS\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensões: 99441 linhas x 8 colunas\n",
            "Colunas disponíveis: ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n"
          ]
        }
      ],
      "source": [
        "orders = dfs['orders_dataset']\n",
        "print(f\"Dimensões: {orders.shape[0]} linhas x {orders.shape[1]} colunas\")\n",
        "print(f\"Colunas disponíveis: {orders.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Informações sobre tipos de dados e valores ausentes:\n",
            "order_id                       | object          |   99,441 únicos |      0 nulos (  0.0%)\n",
            "customer_id                    | object          |   99,441 únicos |      0 nulos (  0.0%)\n",
            "order_status                   | object          |        8 únicos |      0 nulos (  0.0%)\n",
            "order_purchase_timestamp       | object          |   98,875 únicos |      0 nulos (  0.0%)\n",
            "order_approved_at              | object          |   90,733 únicos |    160 nulos (  0.2%)\n",
            "order_delivered_carrier_date   | object          |   81,018 únicos |  1,783 nulos (  1.8%)\n",
            "order_delivered_customer_date  | object          |   95,664 únicos |  2,965 nulos (  3.0%)\n",
            "order_estimated_delivery_date  | object          |      459 únicos |      0 nulos (  0.0%)\n"
          ]
        }
      ],
      "source": [
        "# Verificar tipos de dados e valores ausentes\n",
        "print(\"\\nInformações sobre tipos de dados e valores ausentes:\")\n",
        "for col in orders.columns:\n",
        "    dtype = orders[col].dtype\n",
        "    unique_count = orders[col].nunique()\n",
        "    null_count = orders[col].isnull().sum()\n",
        "    null_pct = (null_count / len(orders)) * 100\n",
        "    print(f\"{col:30} | {str(dtype):15} | {unique_count:8,} únicos | {null_count:6,} nulos ({null_pct:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Distribuição dos status dos pedidos:\n",
            "delivered            | 96,478 ( 97.0%)\n",
            "shipped              |  1,107 (  1.1%)\n",
            "canceled             |    625 (  0.6%)\n",
            "unavailable          |    609 (  0.6%)\n",
            "invoiced             |    314 (  0.3%)\n",
            "processing           |    301 (  0.3%)\n",
            "created              |      5 (  0.0%)\n",
            "approved             |      2 (  0.0%)\n"
          ]
        }
      ],
      "source": [
        "# Analisar distribuição dos status dos pedidos\n",
        "print(f\"\\nDistribuição dos status dos pedidos:\")\n",
        "status_counts = orders['order_status'].value_counts()\n",
        "for status, count in status_counts.items():\n",
        "    pct = (count / len(orders)) * 100\n",
        "    print(f\"{status:20} | {count:6,} ({pct:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensões: 99441 linhas x 8 colunas\n",
            "Colunas disponíveis: ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Dimensões: {orders.shape[0]} linhas x {orders.shape[1]} colunas\")\n",
        "print(f\"Colunas disponíveis: {orders.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Informações sobre tipos de dados e valores ausentes:\n",
            "order_id                       | object          |   99,441 únicos |      0 nulos (  0.0%)\n",
            "customer_id                    | object          |   99,441 únicos |      0 nulos (  0.0%)\n",
            "order_status                   | object          |        8 únicos |      0 nulos (  0.0%)\n",
            "order_purchase_timestamp       | object          |   98,875 únicos |      0 nulos (  0.0%)\n",
            "order_approved_at              | object          |   90,733 únicos |    160 nulos (  0.2%)\n",
            "order_delivered_carrier_date   | object          |   81,018 únicos |  1,783 nulos (  1.8%)\n",
            "order_delivered_customer_date  | object          |   95,664 únicos |  2,965 nulos (  3.0%)\n",
            "order_estimated_delivery_date  | object          |      459 únicos |      0 nulos (  0.0%)\n"
          ]
        }
      ],
      "source": [
        "# Verificar tipos de dados e valores ausentes\n",
        "print(\"\\nInformações sobre tipos de dados e valores ausentes:\")\n",
        "for col in orders.columns:\n",
        "    dtype = orders[col].dtype\n",
        "    unique_count = orders[col].nunique()\n",
        "    null_count = orders[col].isnull().sum()\n",
        "    null_pct = (null_count / len(orders)) * 100\n",
        "    print(f\"{col:30} | {str(dtype):15} | {unique_count:8,} únicos | {null_count:6,} nulos ({null_pct:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Distribuição dos status dos pedidos:\n",
            "delivered            | 96,478 ( 97.0%)\n",
            "shipped              |  1,107 (  1.1%)\n",
            "canceled             |    625 (  0.6%)\n",
            "unavailable          |    609 (  0.6%)\n",
            "invoiced             |    314 (  0.3%)\n",
            "processing           |    301 (  0.3%)\n",
            "created              |      5 (  0.0%)\n",
            "approved             |      2 (  0.0%)\n"
          ]
        }
      ],
      "source": [
        "# Analisar distribuição dos status dos pedidos\n",
        "print(f\"\\nDistribuição dos status dos pedidos:\")\n",
        "status_counts = orders['order_status'].value_counts()\n",
        "for status, count in status_counts.items():\n",
        "    pct = (count / len(orders)) * 100\n",
        "    print(f\"{status:20} | {count:6,} ({pct:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensões: 99441 linhas x 8 colunas\n",
            "Colunas disponíveis: ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n"
          ]
        }
      ],
      "source": [
        "orders = dfs['orders_dataset']\n",
        "print(f\"Dimensões: {orders.shape[0]} linhas x {orders.shape[1]} colunas\")\n",
        "print(f\"Colunas disponíveis: {orders.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Informações sobre tipos de dados e valores ausentes:\n",
            "order_id                       | object          |   99,441 únicos |      0 nulos (  0.0%)\n",
            "customer_id                    | object          |   99,441 únicos |      0 nulos (  0.0%)\n",
            "order_status                   | object          |        8 únicos |      0 nulos (  0.0%)\n",
            "order_purchase_timestamp       | object          |   98,875 únicos |      0 nulos (  0.0%)\n",
            "order_approved_at              | object          |   90,733 únicos |    160 nulos (  0.2%)\n",
            "order_delivered_carrier_date   | object          |   81,018 únicos |  1,783 nulos (  1.8%)\n",
            "order_delivered_customer_date  | object          |   95,664 únicos |  2,965 nulos (  3.0%)\n",
            "order_estimated_delivery_date  | object          |      459 únicos |      0 nulos (  0.0%)\n"
          ]
        }
      ],
      "source": [
        "# Verificar tipos de dados e valores ausentes\n",
        "print(\"\\nInformações sobre tipos de dados e valores ausentes:\")\n",
        "for col in orders.columns:\n",
        "    dtype = orders[col].dtype\n",
        "    unique_count = orders[col].nunique()\n",
        "    null_count = orders[col].isnull().sum()\n",
        "    null_pct = (null_count / len(orders)) * 100\n",
        "    print(f\"{col:30} | {str(dtype):15} | {unique_count:8,} únicos | {null_count:6,} nulos ({null_pct:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Distribuição dos status dos pedidos:\n",
            "delivered            | 96,478 ( 97.0%)\n",
            "shipped              |  1,107 (  1.1%)\n",
            "canceled             |    625 (  0.6%)\n",
            "unavailable          |    609 (  0.6%)\n",
            "invoiced             |    314 (  0.3%)\n",
            "processing           |    301 (  0.3%)\n",
            "created              |      5 (  0.0%)\n",
            "approved             |      2 (  0.0%)\n"
          ]
        }
      ],
      "source": [
        "# Analisar distribuição dos status dos pedidos\n",
        "print(f\"\\nDistribuição dos status dos pedidos:\")\n",
        "status_counts = orders['order_status'].value_counts()\n",
        "for status, count in status_counts.items():\n",
        "    pct = (count / len(orders)) * 100\n",
        "    print(f\"{status:20} | {count:6,} ({pct:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UWV-tvsHyzE"
      },
      "source": [
        "### Análise das Tabelas Complementares\n",
        "\n",
        "Vamos examinar as tabelas que contêm informações específicas sobre diferentes aspectos dos pedidos: itens comprados, avaliações dos clientes e dados de pagamento. Essas tabelas serão fundamentais para a construção de features para nossos modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Análise da tabela de itens dos pedidos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1hMfLY3H0wi",
        "outputId": "b1564b1c-173c-437e-eef1-02c2ee1628c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANÁLISE DA TABELA ORDER_ITEMS\n",
            "==================================================\n",
            "Dimensões: (112650, 7)\n"
          ]
        }
      ],
      "source": [
        "# Análise da tabela de itens dos pedidos\n",
        "print(\"ANÁLISE DA TABELA ORDER_ITEMS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "items = dfs['order_items_dataset']\n",
        "print(f\"Dimensões: {items.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Estatísticas dos valores de preço e frete:\n",
            "               price  freight_value\n",
            "count  112650.000000  112650.000000\n",
            "mean      120.653739      19.990320\n",
            "std       183.633928      15.806405\n",
            "min         0.850000       0.000000\n",
            "25%        39.900000      13.080000\n",
            "50%        74.990000      16.260000\n",
            "75%       134.900000      21.150000\n",
            "max      6735.000000     409.680000\n"
          ]
        }
      ],
      "source": [
        "# Estatísticas descritivas dos valores monetários\n",
        "print(\"\\nEstatísticas dos valores de preço e frete:\")\n",
        "numeric_cols = ['price', 'freight_value']\n",
        "print(items[numeric_cols].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Análise da tabela de reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ANÁLISE DA TABELA REVIEWS\n",
            "==================================================\n",
            "Dimensões: (99224, 7)\n"
          ]
        }
      ],
      "source": [
        "# Análise da tabela de reviews\n",
        "print(\"\\n\\nANÁLISE DA TABELA REVIEWS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "reviews = dfs['order_reviews_dataset']\n",
        "print(f\"Dimensões: {reviews.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Distribuição das notas de avaliação:\n",
            "Nota 1: 11,424 avaliações ( 11.5%)\n",
            "Nota 2:  3,151 avaliações (  3.2%)\n",
            "Nota 3:  8,179 avaliações (  8.2%)\n",
            "Nota 4: 19,142 avaliações ( 19.3%)\n",
            "Nota 5: 57,328 avaliações ( 57.8%)\n"
          ]
        }
      ],
      "source": [
        "# Distribuição das notas de avaliação\n",
        "print(\"\\nDistribuição das notas de avaliação:\")\n",
        "score_dist = reviews['review_score'].value_counts().sort_index()\n",
        "for score, count in score_dist.items():\n",
        "    pct = (count / len(reviews)) * 100\n",
        "    print(f\"Nota {score}: {count:6,} avaliações ({pct:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Reviews com título: 11,568 (11.7%)\n",
            "Reviews com mensagem: 40,977 (41.3%)\n"
          ]
        }
      ],
      "source": [
        "# Verificar reviews com comentários\n",
        "has_title = reviews['review_comment_title'].notna().sum()\n",
        "has_message = reviews['review_comment_message'].notna().sum()\n",
        "print(f\"\\nReviews com título: {has_title:,} ({has_title/len(reviews)*100:.1f}%)\")\n",
        "print(f\"Reviews com mensagem: {has_message:,} ({has_message/len(reviews)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Análise da tabela de pagamentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "ANÁLISE DA TABELA PAYMENTS\n",
            "==================================================\n",
            "Dimensões: (103886, 5)\n"
          ]
        }
      ],
      "source": [
        "# Análise da tabela de pagamentos\n",
        "print(\"\\n\\nANÁLISE DA TABELA PAYMENTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "payments = dfs['order_payments_dataset']\n",
        "print(f\"Dimensões: {payments.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Distribuição dos tipos de pagamento:\n",
            "credit_card     | 76,795 ( 73.9%)\n",
            "boleto          | 19,784 ( 19.0%)\n",
            "voucher         |  5,775 (  5.6%)\n",
            "debit_card      |  1,529 (  1.5%)\n",
            "not_defined     |      3 (  0.0%)\n"
          ]
        }
      ],
      "source": [
        "# Tipos de pagamento\n",
        "print(\"\\nDistribuição dos tipos de pagamento:\")\n",
        "payment_types = payments['payment_type'].value_counts()\n",
        "for ptype, count in payment_types.items():\n",
        "    pct = (count / len(payments)) * 100\n",
        "    print(f\"{ptype:15} | {count:6,} ({pct:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Estatísticas dos valores de pagamento:\n",
            "count    103886.000000\n",
            "mean        154.100380\n",
            "std         217.494064\n",
            "min           0.000000\n",
            "25%          56.790000\n",
            "50%         100.000000\n",
            "75%         171.837500\n",
            "max       13664.080000\n",
            "Name: payment_value, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Estatísticas dos valores de pagamento\n",
        "print(f\"\\nEstatísticas dos valores de pagamento:\")\n",
        "print(payments['payment_value'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Análise das parcelas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Distribuição do número de parcelas:\n",
            " 1 parcelas: 52,546 ( 50.6%)\n",
            " 2 parcelas: 12,413 ( 11.9%)\n",
            " 3 parcelas: 10,461 ( 10.1%)\n",
            " 4 parcelas:  7,098 (  6.8%)\n",
            "10 parcelas:  5,328 (  5.1%)\n",
            " 5 parcelas:  5,239 (  5.0%)\n",
            " 8 parcelas:  4,268 (  4.1%)\n",
            " 6 parcelas:  3,920 (  3.8%)\n",
            " 7 parcelas:  1,626 (  1.6%)\n",
            " 9 parcelas:    644 (  0.6%)\n"
          ]
        }
      ],
      "source": [
        "# Análise das parcelas\n",
        "print(f\"\\nDistribuição do número de parcelas:\")\n",
        "installments_dist = payments['payment_installments'].value_counts().head(10)\n",
        "for inst, count in installments_dist.items():\n",
        "    pct = (count / len(payments)) * 100\n",
        "    print(f\"{inst:2.0f} parcelas: {count:6,} ({pct:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ec-q2fvH22O"
      },
      "source": [
        "### Verificação da Integridade dos Relacionamentos\n",
        "\n",
        "Antes de prosseguir com a definição dos problemas de machine learning, é crucial verificar como as tabelas se relacionam e identificar possíveis inconsistências nos dados. Vamos analisar a cobertura dos relacionamentos e identificar padrões que podem ser relevantes para nossos modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Verificação dos relacionamentos entre tabelas através de order_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYC_gsl5H4ky",
        "outputId": "50c2028c-f757-4bbc-cbe5-a70427fdbf92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VERIFICAÇÃO DE RELACIONAMENTOS ENTRE TABELAS\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Verificação dos relacionamentos entre tabelas através de order_id\n",
        "print(\"VERIFICAÇÃO DE RELACIONAMENTOS ENTRE TABELAS\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Coletar conjuntos de order_ids de cada tabela"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coletar conjuntos de order_ids de cada tabela\n",
        "orders_ids = set(orders['order_id'])\n",
        "items_ids = set(items['order_id'])\n",
        "reviews_ids = set(reviews['order_id'])\n",
        "payments_ids = set(payments['order_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contagem de order_ids únicos por tabela:\n",
            "Orders:   99,441\n",
            "Items:    98,666\n",
            "Reviews:  98,673\n",
            "Payments: 99,440\n"
          ]
        }
      ],
      "source": [
        "print(\"Contagem de order_ids únicos por tabela:\")\n",
        "print(f\"Orders:   {len(orders_ids):6,}\")\n",
        "print(f\"Items:    {len(items_ids):6,}\")\n",
        "print(f\"Reviews:  {len(reviews_ids):6,}\")\n",
        "print(f\"Payments: {len(payments_ids):6,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Verificar intersecções"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Intersecções entre tabelas:\n"
          ]
        }
      ],
      "source": [
        "# Verificar intersecções\n",
        "print(\"\\nIntersecções entre tabelas:\")\n",
        "orders_with_items = len(orders_ids & items_ids)\n",
        "orders_with_reviews = len(orders_ids & reviews_ids)\n",
        "orders_with_payments = len(orders_ids & payments_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orders que têm itens:     98,666 ( 99.2%)\n",
            "Orders que têm reviews:   98,673 ( 99.2%)\n",
            "Orders que têm payments:  99,440 (100.0%)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Orders que têm itens:     {orders_with_items:6,} ({orders_with_items/len(orders_ids)*100:5.1f}%)\")\n",
        "print(f\"Orders que têm reviews:   {orders_with_reviews:6,} ({orders_with_reviews/len(orders_ids)*100:5.1f}%)\")\n",
        "print(f\"Orders que têm payments:  {orders_with_payments:6,} ({orders_with_payments/len(orders_ids)*100:5.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Orders SEM review: 768 (0.8%)\n"
          ]
        }
      ],
      "source": [
        "# Identificar orders sem reviews (pode ser um problema interessante)\n",
        "orders_sem_review = orders_ids - reviews_ids\n",
        "pct_sem_review = len(orders_sem_review) / len(orders_ids) * 100\n",
        "print(f\"\\nOrders SEM review: {len(orders_sem_review):,} ({pct_sem_review:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Distribuição de itens por pedido:\n",
            "count    98666.000000\n",
            "mean         1.141731\n",
            "std          0.538452\n",
            "min          1.000000\n",
            "25%          1.000000\n",
            "50%          1.000000\n",
            "75%          1.000000\n",
            "max         21.000000\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Analisar quantos itens por pedido\n",
        "items_por_pedido = items.groupby('order_id').size()\n",
        "print(f\"\\nDistribuição de itens por pedido:\")\n",
        "print(items_por_pedido.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Pedidos com múltiplos pagamentos: 2,961\n",
            "Distribuição de pagamentos por pedido:\n",
            "1     96479\n",
            "2      2382\n",
            "3       301\n",
            "4       108\n",
            "5        52\n",
            "6        36\n",
            "7        28\n",
            "8        11\n",
            "9         9\n",
            "11        8\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Verificar se há pedidos com múltiplos pagamentos\n",
        "payments_por_pedido = payments.groupby('order_id').size()\n",
        "pedidos_multiplos_pagamentos = (payments_por_pedido > 1).sum()\n",
        "print(f\"\\nPedidos com múltiplos pagamentos: {pedidos_multiplos_pagamentos:,}\")\n",
        "if pedidos_multiplos_pagamentos > 0:\n",
        "    print(\"Distribuição de pagamentos por pedido:\")\n",
        "    print(payments_por_pedido.value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZQGFdqgOSdA"
      },
      "source": [
        "## Definição dos Problemas de Machine Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwZG520QjrHe"
      },
      "source": [
        "Com base na análise exploratória, definimos dois problemas complementares que oferecem valor comercial e desafio técnico adequado ao escopo do projeto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RiJeCInjysX"
      },
      "source": [
        "### Problema 1: Aprendizado Supervisionado (Classificação)\n",
        "**Objetivo**: Predizer a nota de avaliação (review_score) que um cliente dará a um pedido\n",
        "\n",
        "**Target**: review_score (variável categórica ordinal: 1, 2, 3, 4, 5)<br>\n",
        "**Tipo**: Classificação multi-classe com 5 categorias<br>\n",
        "**Desafio**: Distribuição desbalanceada (57.8% classe 5, 11.5% classe 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV_9L_gajydR"
      },
      "source": [
        "\n",
        "**Justificativa**:\n",
        "- Permite à Olist antecipar satisfação do cliente\n",
        "- Identificar pedidos com risco de baixa avaliação\n",
        "- Otimizar estratégias de atendimento pré-entrega\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2kgLjBCj4Dw"
      },
      "source": [
        "### Problema 2: Aprendizado Não Supervisionado (Clustering)\n",
        "\n",
        "**Objetivo**: Segmentar pedidos por comportamento de compra para identificar perfis distintos de consumo\n",
        "\n",
        "**Abordagem**: Clustering baseado em características de comportamento de compra<br>\n",
        "**Algoritmos**: K-Means, DBSCAN, Gaussian Mixture Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-BD_BGcj3NV"
      },
      "source": [
        "**Justificativa**:\n",
        "- Personalização de estratégias de marketing\n",
        "- Otimização de logística por perfil\n",
        "- Identificação de oportunidades de cross-selling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPp2cNRjlEYw"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ2P7y9xdXdn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ESTADO ATUAL DOS DATAFRAMES ===\n",
            "orders: (99441, 8)\n",
            "  Colunas: ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at']...\n",
            "  Valores nulos: 4908\n",
            "\n",
            "items: (112650, 7)\n",
            "  Colunas: ['order_id', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date']...\n",
            "  Valores nulos: 0\n",
            "\n",
            "payments: (103886, 5)\n",
            "  Colunas: ['order_id', 'payment_sequential', 'payment_type', 'payment_installments', 'payment_value']...\n",
            "  Valores nulos: 0\n",
            "\n",
            "reviews: (99224, 7)\n",
            "  Colunas: ['review_id', 'order_id', 'review_score', 'review_comment_title', 'review_comment_message']...\n",
            "  Valores nulos: 145903\n",
            "\n",
            "customers: (99441, 5)\n",
            "  Colunas: ['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state']...\n",
            "  Valores nulos: 0\n",
            "\n",
            "products: (32951, 9)\n",
            "  Colunas: ['product_id', 'product_category_name', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty']...\n",
            "  Valores nulos: 2448\n",
            "\n",
            "sellers: (3095, 4)\n",
            "  Colunas: ['seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state']...\n",
            "  Valores nulos: 0\n",
            "\n",
            "DataFrames organizados com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# Analise dos Dados para Feature Engineering\n",
        "\n",
        "# Verificar quais DataFrames estão carregados e suas dimensões\n",
        "print(\"=== ESTADO ATUAL DOS DATAFRAMES ===\")\n",
        "\n",
        "# Acessar os dataframes do dicionário dfs\n",
        "orders = dfs['orders_dataset']\n",
        "items = dfs['order_items_dataset']\n",
        "payments = dfs['order_payments_dataset']\n",
        "reviews = dfs['order_reviews_dataset']\n",
        "customers = dfs['customers_dataset']\n",
        "products = dfs['products_dataset']\n",
        "sellers = dfs['sellers_dataset']\n",
        "\n",
        "dataframes = {\n",
        "    'orders': orders,\n",
        "    'items': items,\n",
        "    'payments': payments,\n",
        "    'reviews': reviews,\n",
        "    'customers': customers,\n",
        "    'products': products,\n",
        "    'sellers': sellers\n",
        "}\n",
        "\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"{name}: {df.shape}\")\n",
        "    print(f\"  Colunas: {df.columns.tolist()[:5]}...\")  # Primeiras 5 colunas\n",
        "    print(f\"  Valores nulos: {df.isnull().sum().sum()}\")\n",
        "    print()\n",
        "\n",
        "print(\"DataFrames organizados com sucesso!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "nZLgXEjkdaNm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PREPARAÇÃO DAS COLUNAS DE DATA ===\n",
            "✓ order_purchase_timestamp convertida para datetime\n",
            "✓ order_approved_at convertida para datetime\n",
            "✓ order_delivered_carrier_date convertida para datetime\n",
            "✓ order_delivered_customer_date convertida para datetime\n",
            "✓ order_estimated_delivery_date convertida para datetime\n",
            "✓ review_creation_date convertida para datetime\n",
            "✓ review_answer_timestamp convertida para datetime\n",
            "\n",
            "Datasets preparados para feature engineering!\n"
          ]
        }
      ],
      "source": [
        "# Converter colunas de data para datetime\n",
        "print(\"=== PREPARAÇÃO DAS COLUNAS DE DATA ===\")\n",
        "\n",
        "# Colunas de data no dataset orders\n",
        "date_columns = [\n",
        "    'order_purchase_timestamp',\n",
        "    'order_approved_at',\n",
        "    'order_delivered_carrier_date',\n",
        "    'order_delivered_customer_date',\n",
        "    'order_estimated_delivery_date'\n",
        "]\n",
        "\n",
        "for col in date_columns:\n",
        "    if col in orders.columns:\n",
        "        orders[col] = pd.to_datetime(orders[col], errors='coerce')\n",
        "        print(f\"✓ {col} convertida para datetime\")\n",
        "\n",
        "# Verificar se há outros datasets com colunas de data\n",
        "if 'review_creation_date' in reviews.columns:\n",
        "    reviews['review_creation_date'] = pd.to_datetime(reviews['review_creation_date'], errors='coerce')\n",
        "    print(\"✓ review_creation_date convertida para datetime\")\n",
        "\n",
        "if 'review_answer_timestamp' in reviews.columns:\n",
        "    reviews['review_answer_timestamp'] = pd.to_datetime(reviews['review_answer_timestamp'], errors='coerce')\n",
        "    print(\"✓ review_answer_timestamp convertida para datetime\")\n",
        "\n",
        "print(\"\\nDatasets preparados para feature engineering!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st-pBbUPgOCW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DEFININDO ESTRATÉGIA DE LIMPEZA ===\n",
            "PROBLEMAS IDENTIFICADOS E SOLUÇÕES:\n",
            "1. 3% dos pedidos não têm data de entrega (principalmente shipped/canceled)\n",
            "   → Filtrar apenas pedidos 'delivered' para análise\n",
            "2. 880 outliers com tempo entrega >46 dias\n",
            "   → Manter, mas criar flag para análise\n",
            "3. 0.8% dos pedidos sem review\n",
            "   → Para o problema supervisionado, usar apenas com review\n",
            "4. Cobertura boa de items e payments\n",
            "\n",
            "=== IMPLEMENTANDO LIMPEZA ===\n",
            "1. Filtrando pedidos delivered...\n",
            "   Antes: 99,441 | Depois: 96,478\n",
            "2. Removendo pedidos delivered sem data de entrega...\n",
            "   Pedidos finais: 96,470\n"
          ]
        }
      ],
      "source": [
        "# === ESTRATÉGIA DE LIMPEZA DOS DADOS ===\n",
        "print(\"=== DEFININDO ESTRATÉGIA DE LIMPEZA ===\")\n",
        "\n",
        "print(\"PROBLEMAS IDENTIFICADOS E SOLUÇÕES:\")\n",
        "print(\"1. 3% dos pedidos não têm data de entrega (principalmente shipped/canceled)\")\n",
        "print(\"   → Filtrar apenas pedidos 'delivered' para análise\")\n",
        "print(\"2. 880 outliers com tempo entrega >46 dias\")\n",
        "print(\"   → Manter, mas criar flag para análise\")\n",
        "print(\"3. 0.8% dos pedidos sem review\")\n",
        "print(\"   → Para o problema supervisionado, usar apenas com review\")\n",
        "print(\"4. Cobertura boa de items e payments\")\n",
        "\n",
        "print(\"\\n=== IMPLEMENTANDO LIMPEZA ===\")\n",
        "\n",
        "# PASSO 1: Filtrar apenas pedidos delivered\n",
        "print(\"1. Filtrando pedidos delivered...\")\n",
        "orders_clean = orders[orders['order_status'] == 'delivered'].copy()\n",
        "print(f\"   Antes: {len(orders):,} | Depois: {len(orders_clean):,}\")\n",
        "\n",
        "# PASSO 2: Remover pedidos sem data de entrega (mesmo sendo delivered)\n",
        "print(\"2. Removendo pedidos delivered sem data de entrega...\")\n",
        "orders_clean = orders_clean.dropna(subset=['order_delivered_customer_date'])\n",
        "print(f\"   Pedidos finais: {len(orders_clean):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "BsUHMsnqpXRp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Criando features temporais básicas...\n",
            "Features criadas:\n",
            "  dias_para_entrega: 96470 valores\n",
            "  dias_atraso: 96470 valores\n"
          ]
        }
      ],
      "source": [
        "# === CRIANDO FEATURES TEMPORAIS ESSENCIAIS ===\n",
        "print(\"Criando features temporais básicas...\")\n",
        "\n",
        "# Calcular dias para entrega\n",
        "orders_clean['dias_para_entrega'] = (\n",
        "    orders_clean['order_delivered_customer_date'] -\n",
        "    orders_clean['order_purchase_timestamp']\n",
        ").dt.days\n",
        "\n",
        "\n",
        "# Calcular dias de atraso\n",
        "orders_clean['dias_atraso'] = (\n",
        "    orders_clean['order_delivered_customer_date'] -\n",
        "    orders_clean['order_estimated_delivery_date']\n",
        ").dt.days\n",
        "\n",
        "print(f\"Features criadas:\")\n",
        "print(f\"  dias_para_entrega: {orders_clean['dias_para_entrega'].notna().sum()} valores\")\n",
        "print(f\"  dias_atraso: {orders_clean['dias_atraso'].notna().sum()} valores\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3. Criando flags de qualidade...\n",
            "4. Filtrando tabelas relacionadas...\n",
            "   Reviews: 99,224 → 96,353\n",
            "   Items: 112,650 → 110,189\n",
            "   Payments: 103,886 → 100,748\n",
            "5. Verificação final de cobertura...\n",
            "   Orders com review: 95,824 (99.3%)\n",
            "   Orders com items: 96,470 (100.0%)\n",
            "   Orders com payments: 96,469 (100.0%)\n",
            "\n",
            " DADOS LIMPOS PRONTOS PARA FEATURE ENGINEERING!\n",
            "Dataset final: 96,470 pedidos delivered com dados completos\n"
          ]
        }
      ],
      "source": [
        "# PASSO 3: Criar flags para outliers (não remover)\n",
        "print(\"3. Criando flags de qualidade...\")\n",
        "orders_clean['outlier_tempo_entrega'] = (orders_clean['dias_para_entrega'] > 46).astype(int)\n",
        "orders_clean['atraso_extremo'] = (orders_clean['dias_atraso'] > 30).astype(int)\n",
        "orders_clean['entrega_muito_rapida'] = (orders_clean['dias_para_entrega'] < 1).astype(int)\n",
        "\n",
        "# PASSO 4: Filtrar tabelas relacionadas para manter consistência\n",
        "print(\"4. Filtrando tabelas relacionadas...\")\n",
        "order_ids_clean = set(orders_clean['order_id'])\n",
        "\n",
        "# Filtrar reviews, items, payments\n",
        "reviews_clean = reviews[reviews['order_id'].isin(order_ids_clean)].copy()\n",
        "items_clean = items[items['order_id'].isin(order_ids_clean)].copy()\n",
        "payments_clean = payments[payments['order_id'].isin(order_ids_clean)].copy()\n",
        "\n",
        "print(f\"   Reviews: {len(reviews):,} → {len(reviews_clean):,}\")\n",
        "print(f\"   Items: {len(items):,} → {len(items_clean):,}\")\n",
        "print(f\"   Payments: {len(payments):,} → {len(payments_clean):,}\")\n",
        "\n",
        "# PASSO 5: Verificar cobertura final\n",
        "print(\"5. Verificação final de cobertura...\")\n",
        "orders_com_review = orders_clean['order_id'].isin(reviews_clean['order_id']).sum()\n",
        "orders_com_items = orders_clean['order_id'].isin(items_clean['order_id']).sum()\n",
        "orders_com_payments = orders_clean['order_id'].isin(payments_clean['order_id']).sum()\n",
        "\n",
        "print(f\"   Orders com review: {orders_com_review:,} ({orders_com_review/len(orders_clean)*100:.1f}%)\")\n",
        "print(f\"   Orders com items: {orders_com_items:,} ({orders_com_items/len(orders_clean)*100:.1f}%)\")\n",
        "print(f\"   Orders com payments: {orders_com_payments:,} ({orders_com_payments/len(orders_clean)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\n DADOS LIMPOS PRONTOS PARA FEATURE ENGINEERING!\")\n",
        "print(f\"Dataset final: {len(orders_clean):,} pedidos delivered com dados completos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "UbsUMdyjf7NW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ANÁLISE DE QUALIDADE E PROBLEMAS NOS DADOS ===\n",
            "1. PROBLEMAS NA TABELA ORDERS:\n",
            "   Total de pedidos: 99,441\n",
            "\n",
            "   Distribuição de status:\n",
            "     delivered: 96,478 (97.0%)\n",
            "     shipped: 1,107 (1.1%)\n",
            "     canceled: 625 (0.6%)\n",
            "     unavailable: 609 (0.6%)\n",
            "     invoiced: 314 (0.3%)\n",
            "     processing: 301 (0.3%)\n",
            "     created: 5 (0.0%)\n",
            "     approved: 2 (0.0%)\n",
            "\n",
            "   Pedidos SEM data de entrega: 2,965 (3.0%)\n",
            "   Status dos pedidos sem data de entrega:\n",
            "     shipped: 1,107\n",
            "     canceled: 619\n",
            "     unavailable: 609\n",
            "     invoiced: 314\n",
            "     processing: 301\n",
            "     delivered: 8\n",
            "     created: 5\n",
            "     approved: 2\n",
            "\n",
            "2. ANÁLISE DE OUTLIERS TEMPORAIS:\n",
            "   Pedidos entregues para análise: 96,470\n",
            "   Outliers tempo entrega (>P99=46 dias): 880\n",
            "   Pedidos com atraso >30 dias: 345\n",
            "\n",
            "3. COBERTURA DE REVIEWS:\n",
            "   Pedidos com review: 98,673 (99.2%)\n",
            "   Pedidos SEM review: 768\n",
            "\n",
            "4. VERIFICAÇÃO DE RELACIONAMENTOS:\n",
            "   Orders únicos: 99,441\n",
            "   Reviews únicos: 98,673\n",
            "   Items únicos: 98,666\n",
            "   Payments únicos: 99,440\n"
          ]
        }
      ],
      "source": [
        "# === ANÁLISE DE QUALIDADE DOS DADOS ===\n",
        "print(\"=== ANÁLISE DE QUALIDADE E PROBLEMAS NOS DADOS ===\")\n",
        "\n",
        "# 1. ANÁLISE DA TABELA ORDERS\n",
        "print(\"1. PROBLEMAS NA TABELA ORDERS:\")\n",
        "print(f\"   Total de pedidos: {len(orders):,}\")\n",
        "\n",
        "# Status dos pedidos\n",
        "status_counts = orders['order_status'].value_counts()\n",
        "print(f\"\\n   Distribuição de status:\")\n",
        "for status, count in status_counts.items():\n",
        "    pct = count/len(orders)*100\n",
        "    print(f\"     {status}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Pedidos sem data de entrega\n",
        "sem_entrega = orders['order_delivered_customer_date'].isnull().sum()\n",
        "print(f\"\\n   Pedidos SEM data de entrega: {sem_entrega:,} ({sem_entrega/len(orders)*100:.1f}%)\")\n",
        "\n",
        "# Verificar status dos pedidos sem entrega\n",
        "print(\"   Status dos pedidos sem data de entrega:\")\n",
        "sem_entrega_status = orders[orders['order_delivered_customer_date'].isnull()]['order_status'].value_counts()\n",
        "for status, count in sem_entrega_status.items():\n",
        "    print(f\"     {status}: {count:,}\")\n",
        "\n",
        "# 2. ANÁLISE DE OUTLIERS NAS FEATURES TEMPORAIS\n",
        "print(f\"\\n2. ANÁLISE DE OUTLIERS TEMPORAIS:\")\n",
        "delivered_orders = orders_clean.dropna(subset=['order_delivered_customer_date'])\n",
        "print(f\"   Pedidos entregues para análise: {len(delivered_orders):,}\")\n",
        "\n",
        "# Outliers de tempo de entrega\n",
        "q99_entrega = delivered_orders['dias_para_entrega'].quantile(0.99)\n",
        "outliers_entrega = (delivered_orders['dias_para_entrega'] > q99_entrega).sum()\n",
        "print(f\"   Outliers tempo entrega (>P99={q99_entrega:.0f} dias): {outliers_entrega:,}\")\n",
        "\n",
        "# Outliers de atraso\n",
        "atraso_extremo = delivered_orders['dias_atraso'] > 30  # 30 dias de atraso\n",
        "outliers_atraso = atraso_extremo.sum()\n",
        "print(f\"   Pedidos com atraso >30 dias: {outliers_atraso:,}\")\n",
        "\n",
        "# 3. COBERTURA DE REVIEWS\n",
        "print(f\"\\n3. COBERTURA DE REVIEWS:\")\n",
        "orders_com_review = orders['order_id'].isin(reviews['order_id']).sum()\n",
        "print(f\"   Pedidos com review: {orders_com_review:,} ({orders_com_review/len(orders)*100:.1f}%)\")\n",
        "print(f\"   Pedidos SEM review: {len(orders) - orders_com_review:,}\")\n",
        "\n",
        "# 4. VERIFICAR RELACIONAMENTOS\n",
        "print(f\"\\n4. VERIFICAÇÃO DE RELACIONAMENTOS:\")\n",
        "print(f\"   Orders únicos: {orders['order_id'].nunique():,}\")\n",
        "print(f\"   Reviews únicos: {reviews['order_id'].nunique():,}\")\n",
        "print(f\"   Items únicos: {items['order_id'].nunique():,}\")\n",
        "print(f\"   Payments únicos: {payments['order_id'].nunique():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Te70Ra3zgxO-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== VALIDAÇÃO FINAL DOS DADOS LIMPOS ===\n",
            "1. QUALIDADE DAS FEATURES TEMPORAIS:\n",
            "   dias_para_entrega - valores nulos: 0\n",
            "   dias_atraso - valores nulos: 0\n",
            "   Valores negativos em dias_para_entrega: 0\n",
            "\n",
            "2. DISTRIBUIÇÃO DAS FLAGS DE QUALIDADE:\n",
            "   outlier_tempo_entrega: 880 (0.9%)\n",
            "   atraso_extremo: 345 (0.4%)\n",
            "   entrega_muito_rapida: 13 (0.0%)\n",
            "\n",
            "3. INTEGRIDADE DOS RELACIONAMENTOS:\n",
            "   Orders únicos: 96,470\n",
            "   Intersecção orders ∩ reviews: 95,824\n",
            "   Intersecção orders ∩ items: 96,470\n",
            "   Intersecção orders ∩ payments: 96,469\n",
            "\n",
            "4. ESTATÍSTICAS FINAIS (DADOS LIMPOS):\n",
            "   Tempo de entrega (dias):\n",
            "     Média: 12.1\n",
            "     Mediana: 10.0\n",
            "     P95: 29.0\n",
            "   Atraso (dias):\n",
            "     Média: -11.9\n",
            "     % com atraso (>0): 6.8%\n",
            "\n",
            " DADOS VALIDADOS - PRONTOS PARA FEATURE ENGINEERING!\n",
            "\n",
            "5. SALVANDO PONTO DE CONTROLE...\n",
            "   Variáveis principais atualizadas: orders_clean, reviews_clean, items_clean, payments_clean\n"
          ]
        }
      ],
      "source": [
        "# === VALIDAÇÃO FINAL DOS DADOS LIMPOS ===\n",
        "print(\"=== VALIDAÇÃO FINAL DOS DADOS LIMPOS ===\")\n",
        "\n",
        "# 1. Verificar se todas as variáveis temporais são válidas agora\n",
        "print(\"1. QUALIDADE DAS FEATURES TEMPORAIS:\")\n",
        "print(f\"   dias_para_entrega - valores nulos: {orders_clean['dias_para_entrega'].isnull().sum()}\")\n",
        "print(f\"   dias_atraso - valores nulos: {orders_clean['dias_atraso'].isnull().sum()}\")\n",
        "print(f\"   Valores negativos em dias_para_entrega: {(orders_clean['dias_para_entrega'] < 0).sum()}\")\n",
        "\n",
        "# 2. Distribuição das flags de qualidade\n",
        "print(f\"\\n2. DISTRIBUIÇÃO DAS FLAGS DE QUALIDADE:\")\n",
        "for flag in ['outlier_tempo_entrega', 'atraso_extremo', 'entrega_muito_rapida']:\n",
        "    count = orders_clean[flag].sum()\n",
        "    pct = count / len(orders_clean) * 100\n",
        "    print(f\"   {flag}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# 3. Verificar integridade dos relacionamentos\n",
        "print(f\"\\n3. INTEGRIDADE DOS RELACIONAMENTOS:\")\n",
        "orders_ids = set(orders_clean['order_id'])\n",
        "reviews_ids = set(reviews_clean['order_id'])\n",
        "items_ids = set(items_clean['order_id'])\n",
        "payments_ids = set(payments_clean['order_id'])\n",
        "\n",
        "print(f\"   Orders únicos: {len(orders_ids):,}\")\n",
        "print(f\"   Intersecção orders ∩ reviews: {len(orders_ids & reviews_ids):,}\")\n",
        "print(f\"   Intersecção orders ∩ items: {len(orders_ids & items_ids):,}\")\n",
        "print(f\"   Intersecção orders ∩ payments: {len(orders_ids & payments_ids):,}\")\n",
        "\n",
        "# 4. Estatísticas finais das features temporais limpas\n",
        "print(f\"\\n4. ESTATÍSTICAS FINAIS (DADOS LIMPOS):\")\n",
        "print(f\"   Tempo de entrega (dias):\")\n",
        "print(f\"     Média: {orders_clean['dias_para_entrega'].mean():.1f}\")\n",
        "print(f\"     Mediana: {orders_clean['dias_para_entrega'].median():.1f}\")\n",
        "print(f\"     P95: {orders_clean['dias_para_entrega'].quantile(0.95):.1f}\")\n",
        "\n",
        "print(f\"   Atraso (dias):\")\n",
        "print(f\"     Média: {orders_clean['dias_atraso'].mean():.1f}\")\n",
        "print(f\"     % com atraso (>0): {(orders_clean['dias_atraso'] > 0).mean()*100:.1f}%\")\n",
        "\n",
        "print(f\"\\n DADOS VALIDADOS - PRONTOS PARA FEATURE ENGINEERING!\")\n",
        "\n",
        "# 5. Salvar ponto de controle das variáveis limpas\n",
        "print(f\"\\n5. SALVANDO PONTO DE CONTROLE...\")\n",
        "print(f\"   Variáveis principais atualizadas: orders_clean, reviews_clean, items_clean, payments_clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "KKLTodD9hh2P"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== FEATURE ENGINEERING COMPLETA ===\n",
            "Estratégia: Criar TODAS as features primeiro, depois especializar por problema\n",
            "- Supervisionado: usar features + reviews (excluir próprio review_score)\n",
            "- Clustering: usar features sem qualquer informação de review\n",
            "\n",
            "1. CRIANDO FEATURES DE COMPORTAMENTO DE COMPRA POR PEDIDO...\n",
            "   1.1 Agregando dados dos itens...\n",
            "   1.2 Criando features derivadas relevantes...\n",
            "      ✓ 96,470 pedidos com features de itens\n",
            "   1.3 Agregando dados dos pagamentos...\n",
            "      ✓ 96,469 pedidos com features de pagamento\n",
            "   1.4 Criando features de tipos de pagamento...\n",
            "      ✓ Features de tipos de pagamento criadas\n",
            "\n",
            " RESUMO PARTE 1:\n",
            "   Items features: (96470, 15)\n",
            "   Payments features: (96469, 7)\n",
            "   Payment types: (96469, 5)\n",
            " Features de comportamento de compra criadas!\n"
          ]
        }
      ],
      "source": [
        "# === FEATURE ENGINEERING COMPLETA - ESTRATÉGIA UNIFICADA ===\n",
        "print(\"=== FEATURE ENGINEERING COMPLETA ===\")\n",
        "print(\"Estratégia: Criar TODAS as features primeiro, depois especializar por problema\")\n",
        "print(\"- Supervisionado: usar features + reviews (excluir próprio review_score)\")\n",
        "print(\"- Clustering: usar features sem qualquer informação de review\")\n",
        "print()\n",
        "\n",
        "# === PARTE 1: FEATURES AGREGADAS POR PEDIDO (COMPORTAMENTO DE COMPRA) ===\n",
        "print(\"1. CRIANDO FEATURES DE COMPORTAMENTO DE COMPRA POR PEDIDO...\")\n",
        "\n",
        "# 1.1 Agregações dos ITENS por pedido (CORRIGIDA)\n",
        "print(\"   1.1 Agregando dados dos itens...\")\n",
        "\n",
        "# Função para desvio padrão seguro\n",
        "def safe_std(x):\n",
        "    if len(x) <= 1:\n",
        "        return 0.0\n",
        "    return x.std()\n",
        "\n",
        "items_features = items_clean.groupby('order_id').agg({\n",
        "    'order_item_id': 'count',\n",
        "    'product_id': 'nunique',\n",
        "    'seller_id': 'nunique',\n",
        "    'price': ['sum', 'mean', safe_std, 'min', 'max'],\n",
        "    'freight_value': ['sum', 'mean', safe_std]\n",
        "}).round(2)\n",
        "\n",
        "# Simplificar nomes das colunas\n",
        "items_features.columns = [\n",
        "    'qtd_itens', 'qtd_produtos_unicos', 'qtd_vendedores',\n",
        "    'valor_total_produtos', 'valor_medio_item', 'std_valor_item', 'valor_min_item', 'valor_max_item',\n",
        "    'valor_total_frete', 'valor_medio_frete', 'std_valor_frete'\n",
        "]\n",
        "# 1.2 Features derivadas importantes para AMBOS problemas\n",
        "print(\"   1.2 Criando features derivadas relevantes...\")\n",
        "items_features['ticket_medio'] = (items_features['valor_total_produtos'] / items_features['qtd_itens']).round(2)\n",
        "items_features['frete_percentual'] = (items_features['valor_total_frete'] /\n",
        "                                     (items_features['valor_total_produtos'] + items_features['valor_total_frete']) * 100).round(2)\n",
        "items_features['pedido_multi_vendedor'] = (items_features['qtd_vendedores'] > 1).astype(int)\n",
        "items_features['diversidade_produtos'] = (items_features['qtd_produtos_unicos'] / items_features['qtd_itens']).round(2)\n",
        "\n",
        "print(f\"      ✓ {items_features.shape[0]:,} pedidos com features de itens\")\n",
        "\n",
        "# 1.3 Agregações dos PAGAMENTOS por pedido\n",
        "print(\"   1.3 Agregando dados dos pagamentos...\")\n",
        "payments_features = payments_clean.groupby('order_id').agg({\n",
        "    'payment_sequential': 'count',               # Quantidade de formas de pagamento\n",
        "    'payment_installments': ['sum', 'mean', 'max'], # Estatísticas de parcelas\n",
        "    'payment_value': ['sum', 'mean', 'std']      # Estatísticas de valor pago\n",
        "}).round(2)\n",
        "\n",
        "payments_features.columns = [\n",
        "    'qtd_formas_pagamento',\n",
        "    'total_parcelas', 'media_parcelas', 'max_parcelas',\n",
        "    'valor_total_pago', 'valor_medio_pagamento', 'std_valor_pagamento'\n",
        "]\n",
        "\n",
        "print(f\"      ✓ {payments_features.shape[0]:,} pedidos com features de pagamento\")\n",
        "\n",
        "# 1.4 Tipos de pagamento (importantes para segmentação)\n",
        "print(\"   1.4 Criando features de tipos de pagamento...\")\n",
        "payment_types = payments_clean.groupby('order_id')['payment_type'].apply(list).reset_index()\n",
        "payment_types['usa_credit_card'] = payment_types['payment_type'].apply(lambda x: 1 if 'credit_card' in x else 0)\n",
        "payment_types['usa_boleto'] = payment_types['payment_type'].apply(lambda x: 1 if 'boleto' in x else 0)\n",
        "payment_types['usa_debit_card'] = payment_types['payment_type'].apply(lambda x: 1 if 'debit_card' in x else 0)\n",
        "payment_types['usa_voucher'] = payment_types['payment_type'].apply(lambda x: 1 if 'voucher' in x else 0)\n",
        "payment_types['diversidade_pagamento'] = payment_types['payment_type'].apply(lambda x: len(set(x)))\n",
        "payment_types = payment_types.drop('payment_type', axis=1).set_index('order_id')\n",
        "\n",
        "print(f\"      ✓ Features de tipos de pagamento criadas\")\n",
        "\n",
        "print(f\"\\n RESUMO PARTE 1:\")\n",
        "print(f\"   Items features: {items_features.shape}\")\n",
        "print(f\"   Payments features: {payments_features.shape}\")\n",
        "print(f\"   Payment types: {payment_types.shape}\")\n",
        "print(f\" Features de comportamento de compra criadas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "afd0eVasiXMK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== CONCLUSÃO DA INVESTIGAÇÃO ===\n",
            "DESCOBERTAS IMPORTANTES:\n",
            "1. Cada 'customer_id' realmente tem apenas 1 pedido (100%)\n",
            "2. MAS: customer_unique_id mostra o padrão REAL de recorrência:\n",
            "   - 96.9% clientes com 1 pedido\n",
            "   - 2.9% clientes com 2 pedidos\n",
            "   - 0.2% clientes com 3+ pedidos\n",
            "3. Nossa limpeza eliminou justamente os pedidos recorrentes!\n",
            "   (provavelmente pedidos shipped/canceled de clientes recorrentes)\n",
            "\n",
            "IMPLICAÇÃO PARA FEATURE ENGINEERING:\n",
            "- customer_id = identificador único por pedido\n",
            "- customer_unique_id = identificador real do cliente\n",
            "- Precisamos REFAZER features de cliente usando customer_unique_id\n",
            "\n",
            "=== CORRIGINDO FEATURES DE COMPORTAMENTO DO CLIENTE ===\n",
            "1. Recalculando com customer_unique_id...\n",
            "2. ESTATÍSTICAS CORRIGIDAS:\n",
            "   Clientes únicos reais: 93,350\n",
            "   Clientes recorrentes: 2,801 (3.0%)\n",
            "   Pedidos médios por cliente: 1.03\n",
            "   Max pedidos por cliente: 15\n",
            "\n",
            "3. DISTRIBUIÇÃO CORRIGIDA DE PEDIDOS POR CLIENTE:\n",
            "   1 pedido(s): 90,549 clientes (97.0%)\n",
            "   2 pedido(s): 2,573 clientes (2.8%)\n",
            "   3 pedido(s): 181 clientes (0.2%)\n",
            "   4 pedido(s): 28 clientes (0.0%)\n",
            "   5 pedido(s): 9 clientes (0.0%)\n",
            "   6 pedido(s): 5 clientes (0.0%)\n",
            "   7 pedido(s): 3 clientes (0.0%)\n",
            "   9 pedido(s): 1 clientes (0.0%)\n",
            "\n",
            "ATUALIZAÇÃO CONCLUÍDA:\n",
            "   Features de cliente corrigidas: (96470, 4)\n",
            "   Agora refletem o comportamento real de recorrência!\n"
          ]
        }
      ],
      "source": [
        "# === CONCLUSÃO DA INVESTIGAÇÃO E CORREÇÃO ===\n",
        "print(\"=== CONCLUSÃO DA INVESTIGAÇÃO ===\")\n",
        "print(\"DESCOBERTAS IMPORTANTES:\")\n",
        "print(\"1. Cada 'customer_id' realmente tem apenas 1 pedido (100%)\")\n",
        "print(\"2. MAS: customer_unique_id mostra o padrão REAL de recorrência:\")\n",
        "print(\"   - 96.9% clientes com 1 pedido\")\n",
        "print(\"   - 2.9% clientes com 2 pedidos\")\n",
        "print(\"   - 0.2% clientes com 3+ pedidos\")\n",
        "print(\"3. Nossa limpeza eliminou justamente os pedidos recorrentes!\")\n",
        "print(\"   (provavelmente pedidos shipped/canceled de clientes recorrentes)\")\n",
        "\n",
        "print(\"\\nIMPLICAÇÃO PARA FEATURE ENGINEERING:\")\n",
        "print(\"- customer_id = identificador único por pedido\")\n",
        "print(\"- customer_unique_id = identificador real do cliente\")\n",
        "print(\"- Precisamos REFAZER features de cliente usando customer_unique_id\")\n",
        "\n",
        "print(\"\\n=== CORRIGINDO FEATURES DE COMPORTAMENTO DO CLIENTE ===\")\n",
        "\n",
        "# CORREÇÃO: Usar customer_unique_id para calcular recorrência real\n",
        "print(\"1. Recalculando com customer_unique_id...\")\n",
        "\n",
        "# Merge para obter customer_unique_id nos pedidos limpos\n",
        "orders_corrected = orders_clean.merge(\n",
        "    customers[['customer_id', 'customer_unique_id']],\n",
        "    on='customer_id', how='left'\n",
        ")\n",
        "\n",
        "# Recalcular comportamento por customer_unique_id\n",
        "customer_behavior_corrected = orders_corrected.groupby('customer_unique_id').agg({\n",
        "    'order_id': 'count',\n",
        "    'order_purchase_timestamp': ['min', 'max']\n",
        "}).round(2)\n",
        "\n",
        "customer_behavior_corrected.columns = ['total_pedidos_cliente', 'primeira_compra', 'ultima_compra']\n",
        "\n",
        "# Tempo como cliente e frequência\n",
        "customer_behavior_corrected['dias_como_cliente'] = (\n",
        "    customer_behavior_corrected['ultima_compra'] - customer_behavior_corrected['primeira_compra']\n",
        ").dt.days.fillna(0)\n",
        "\n",
        "customer_behavior_corrected['frequencia_compra_mensal'] = (\n",
        "    customer_behavior_corrected['total_pedidos_cliente'] /\n",
        "    (customer_behavior_corrected['dias_como_cliente'] / 30 + 1)\n",
        ").round(3)\n",
        "\n",
        "# Classificações corrigidas\n",
        "customer_behavior_corrected['cliente_recorrente'] = (customer_behavior_corrected['total_pedidos_cliente'] > 1).astype(int)\n",
        "customer_behavior_corrected['cliente_frequente'] = (customer_behavior_corrected['total_pedidos_cliente'] >= 3).astype(int)\n",
        "\n",
        "# Mapear de volta para os pedidos\n",
        "customer_features_corrected = orders_corrected[['order_id', 'customer_unique_id']].merge(\n",
        "    customer_behavior_corrected[['total_pedidos_cliente', 'cliente_recorrente', 'frequencia_compra_mensal']],\n",
        "    left_on='customer_unique_id', right_index=True\n",
        ").set_index('order_id')\n",
        "\n",
        "print(f\"2. ESTATÍSTICAS CORRIGIDAS:\")\n",
        "print(f\"   Clientes únicos reais: {customer_behavior_corrected.shape[0]:,}\")\n",
        "print(f\"   Clientes recorrentes: {customer_behavior_corrected['cliente_recorrente'].sum():,} ({customer_behavior_corrected['cliente_recorrente'].mean()*100:.1f}%)\")\n",
        "print(f\"   Pedidos médios por cliente: {customer_behavior_corrected['total_pedidos_cliente'].mean():.2f}\")\n",
        "print(f\"   Max pedidos por cliente: {customer_behavior_corrected['total_pedidos_cliente'].max()}\")\n",
        "\n",
        "print(f\"\\n3. DISTRIBUIÇÃO CORRIGIDA DE PEDIDOS POR CLIENTE:\")\n",
        "dist_corrected = customer_behavior_corrected['total_pedidos_cliente'].value_counts().sort_index()\n",
        "for pedidos, count in dist_corrected.head(8).items():\n",
        "    pct = count / len(customer_behavior_corrected) * 100\n",
        "    print(f\"   {pedidos} pedido(s): {count:,} clientes ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\nATUALIZAÇÃO CONCLUÍDA:\")\n",
        "print(f\"   Features de cliente corrigidas: {customer_features_corrected.shape}\")\n",
        "print(\"   Agora refletem o comportamento real de recorrência!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "HgWrzdLHhsqy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2. CRIANDO FEATURES DE COMPORTAMENTO HISTÓRICO DO CLIENTE...\n",
            "   CORREÇÃO: Usando customer_unique_id para capturar recorrência real\n",
            "   2.1 Preparando dados com identificação correta de clientes...\n",
            "   2.2 Calculando features de recência e frequência...\n",
            "   2.3 Calculando tempo como cliente e frequência...\n",
            "   2.4 Classificando tipos de cliente...\n",
            "   2.5 Calculando posição na jornada do cliente...\n",
            "   Resultado: 96,470 pedidos com features de cliente\n",
            "\n",
            "   ESTATISTICAS DE COMPORTAMENTO (CORRIGIDAS):\n",
            "   Clientes únicos reais: 93,350\n",
            "   Clientes recorrentes: 2,801 (3.0%)\n",
            "   Clientes frequentes (3+): 228 (0.2%)\n",
            "   Pedidos médios por cliente: 1.03\n",
            "   Máximo de pedidos por cliente: 15\n",
            "\n",
            "   DISTRIBUIÇÃO DE PEDIDOS POR CLIENTE:\n",
            "     1 pedido(s): 90,549 clientes (97.0%)\n",
            "     2 pedido(s): 2,573 clientes (2.8%)\n",
            "     3 pedido(s): 181 clientes (0.2%)\n",
            "     4 pedido(s): 28 clientes (0.0%)\n",
            "     5 pedido(s): 9 clientes (0.0%)\n",
            "     6 pedido(s): 5 clientes (0.0%)\n",
            "     7 pedido(s): 3 clientes (0.0%)\n",
            "     9 pedido(s): 1 clientes (0.0%)\n",
            "✓ customer_features definido: (96470, 4)\n",
            "\n",
            " RESUMO PARTE 2 (CORRIGIDA):\n",
            "   Customer features: (96470, 4)\n"
          ]
        }
      ],
      "source": [
        "# === PARTE 2: FEATURES DE COMPORTAMENTO HISTÓRICO DO CLIENTE (CORRIGIDA) ===\n",
        "print(\"2. CRIANDO FEATURES DE COMPORTAMENTO HISTÓRICO DO CLIENTE...\")\n",
        "print(\"   CORREÇÃO: Usando customer_unique_id para capturar recorrência real\")\n",
        "\n",
        "# 2.1 Merge para obter customer_unique_id correto\n",
        "print(\"   2.1 Preparando dados com identificação correta de clientes...\")\n",
        "orders_with_unique = orders_clean.merge(\n",
        "    customers[['customer_id', 'customer_unique_id']],\n",
        "    on='customer_id', how='left'\n",
        ")\n",
        "\n",
        "orders_temporal = orders_with_unique[['order_id', 'customer_unique_id', 'order_purchase_timestamp']].copy()\n",
        "orders_temporal = orders_temporal.sort_values(['customer_unique_id', 'order_purchase_timestamp'])\n",
        "\n",
        "# 2.2 Calcular features de recência e frequência por cliente REAL\n",
        "print(\"   2.2 Calculando features de recência e frequência...\")\n",
        "customer_behavior = orders_temporal.groupby('customer_unique_id').agg({\n",
        "    'order_id': 'count',                        # Total de pedidos do cliente\n",
        "    'order_purchase_timestamp': ['min', 'max']  # Primeira e última compra\n",
        "}).round(2)\n",
        "\n",
        "customer_behavior.columns = ['total_pedidos_cliente', 'primeira_compra', 'ultima_compra']\n",
        "\n",
        "# 2.3 Calcular tempo como cliente e frequência de compra\n",
        "print(\"   2.3 Calculando tempo como cliente e frequência...\")\n",
        "customer_behavior['dias_como_cliente'] = (\n",
        "    customer_behavior['ultima_compra'] - customer_behavior['primeira_compra']\n",
        ").dt.days.fillna(0)\n",
        "\n",
        "# Frequência de compra (pedidos por mês)\n",
        "customer_behavior['frequencia_compra_mensal'] = (\n",
        "    customer_behavior['total_pedidos_cliente'] /\n",
        "    (customer_behavior['dias_como_cliente'] / 30 + 1)  # +1 para evitar divisão por zero\n",
        ").round(3)\n",
        "\n",
        "# 2.4 Classificar tipos de cliente\n",
        "print(\"   2.4 Classificando tipos de cliente...\")\n",
        "customer_behavior['cliente_recorrente'] = (customer_behavior['total_pedidos_cliente'] > 1).astype(int)\n",
        "customer_behavior['cliente_frequente'] = (customer_behavior['total_pedidos_cliente'] >= 3).astype(int)\n",
        "\n",
        "# 2.5 Para cada pedido, calcular posição na jornada do cliente\n",
        "print(\"   2.5 Calculando posição na jornada do cliente...\")\n",
        "orders_temporal['pedido_numero'] = orders_temporal.groupby('customer_unique_id').cumcount() + 1\n",
        "orders_temporal['is_primeiro_pedido'] = (orders_temporal['pedido_numero'] == 1).astype(int)\n",
        "\n",
        "# Merge com dados do cliente\n",
        "customer_features = orders_temporal[['order_id', 'customer_unique_id', 'pedido_numero', 'is_primeiro_pedido']].merge(\n",
        "    customer_behavior[['total_pedidos_cliente', 'frequencia_compra_mensal', 'cliente_recorrente', 'cliente_frequente']],\n",
        "    left_on='customer_unique_id', right_index=True\n",
        ").set_index('order_id')\n",
        "\n",
        "print(f\"   Resultado: {customer_features.shape[0]:,} pedidos com features de cliente\")\n",
        "\n",
        "# 2.6 Estatísticas de comportamento do cliente CORRETAS\n",
        "print(f\"\\n   ESTATISTICAS DE COMPORTAMENTO (CORRIGIDAS):\")\n",
        "print(f\"   Clientes únicos reais: {customer_behavior.shape[0]:,}\")\n",
        "print(f\"   Clientes recorrentes: {customer_behavior['cliente_recorrente'].sum():,} ({customer_behavior['cliente_recorrente'].mean()*100:.1f}%)\")\n",
        "print(f\"   Clientes frequentes (3+): {customer_behavior['cliente_frequente'].sum():,} ({customer_behavior['cliente_frequente'].mean()*100:.1f}%)\")\n",
        "print(f\"   Pedidos médios por cliente: {customer_behavior['total_pedidos_cliente'].mean():.2f}\")\n",
        "print(f\"   Máximo de pedidos por cliente: {customer_behavior['total_pedidos_cliente'].max()}\")\n",
        "\n",
        "# Distribuição correta\n",
        "print(f\"\\n   DISTRIBUIÇÃO DE PEDIDOS POR CLIENTE:\")\n",
        "dist = customer_behavior['total_pedidos_cliente'].value_counts().sort_index()\n",
        "for pedidos, count in dist.head(8).items():\n",
        "    pct = count / len(customer_behavior) * 100\n",
        "    print(f\"     {pedidos} pedido(s): {count:,} clientes ({pct:.1f}%)\")\n",
        "\n",
        "# IMPORTANTE: Definir customer_features para uso posterior\n",
        "customer_features = customer_features_corrected.copy()\n",
        "print(f\"✓ customer_features definido: {customer_features.shape}\")\n",
        "\n",
        "print(f\"\\n RESUMO PARTE 2 (CORRIGIDA):\")\n",
        "print(f\"   Customer features: {customer_features.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "C18qqlKCiEZm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== INVESTIGANDO RECORRÊNCIA DE CLIENTES ===\n",
            "Verificando se a baixa recorrência é real ou artefato da limpeza\n",
            "1. ANÁLISE DOS DADOS ORIGINAIS (antes da limpeza):\n",
            "   Clientes únicos (dados originais): 99,441\n",
            "   Distribuição de pedidos por cliente (original):\n",
            "     1 pedido(s): 99,441 clientes (100.0%)\n",
            "\n",
            "2. VERIFICAÇÃO DE IDs DE CLIENTE:\n",
            "   customer_id únicos: 99,441\n",
            "   customer_unique_id únicos: 96,096\n",
            "   Diferença: 3,345\n",
            "\n",
            "3. ANÁLISE POR CUSTOMER_UNIQUE_ID:\n",
            "   Clientes únicos (por unique_id): 96,096\n",
            "   Distribuição por customer_unique_id:\n",
            "     1 pedido(s): 93,099 clientes (96.9%)\n",
            "     2 pedido(s): 2,745 clientes (2.9%)\n",
            "     3 pedido(s): 203 clientes (0.2%)\n",
            "     4 pedido(s): 30 clientes (0.0%)\n",
            "     5 pedido(s): 8 clientes (0.0%)\n",
            "     6 pedido(s): 6 clientes (0.0%)\n",
            "     7 pedido(s): 3 clientes (0.0%)\n",
            "     9 pedido(s): 1 clientes (0.0%)\n",
            "     17 pedido(s): 1 clientes (0.0%)\n",
            "\n",
            "4. IMPACTO DA LIMPEZA NA RECORRÊNCIA:\n",
            "   Clientes após limpeza: 96,470\n",
            "   Clientes com >1 pedido (após limpeza): 0\n",
            "   Customer_ids perdidos na limpeza: 2,971\n",
            "\n",
            "CONCLUSÃO DA INVESTIGAÇÃO:\n"
          ]
        }
      ],
      "source": [
        "# === INVESTIGAÇÃO: VERIFICAÇÃO DE RECORRÊNCIA DE CLIENTES ===\n",
        "print(\"=== INVESTIGANDO RECORRÊNCIA DE CLIENTES ===\")\n",
        "print(\"Verificando se a baixa recorrência é real ou artefato da limpeza\")\n",
        "\n",
        "# 1. Verificar nos dados ORIGINAIS (antes da limpeza)\n",
        "print(\"1. ANÁLISE DOS DADOS ORIGINAIS (antes da limpeza):\")\n",
        "original_customers = orders.groupby('customer_id').agg({\n",
        "    'order_id': 'count',\n",
        "    'order_status': lambda x: list(x.unique())\n",
        "}).rename(columns={'order_id': 'total_pedidos'})\n",
        "\n",
        "print(f\"   Clientes únicos (dados originais): {len(original_customers):,}\")\n",
        "print(f\"   Distribuição de pedidos por cliente (original):\")\n",
        "recurrence_dist = original_customers['total_pedidos'].value_counts().sort_index()\n",
        "for pedidos, count in recurrence_dist.head(10).items():\n",
        "    pct = count / len(original_customers) * 100\n",
        "    print(f\"     {pedidos} pedido(s): {count:,} clientes ({pct:.1f}%)\")\n",
        "\n",
        "# 2. Verificar customer_unique_id vs customer_id\n",
        "print(f\"\\n2. VERIFICAÇÃO DE IDs DE CLIENTE:\")\n",
        "print(f\"   customer_id únicos: {customers['customer_id'].nunique():,}\")\n",
        "print(f\"   customer_unique_id únicos: {customers['customer_unique_id'].nunique():,}\")\n",
        "print(f\"   Diferença: {customers['customer_id'].nunique() - customers['customer_unique_id'].nunique():,}\")\n",
        "\n",
        "# 3. Analisar se customer_unique_id mostra padrão diferente\n",
        "print(f\"\\n3. ANÁLISE POR CUSTOMER_UNIQUE_ID:\")\n",
        "orders_with_unique = orders.merge(customers[['customer_id', 'customer_unique_id']], on='customer_id')\n",
        "unique_customers = orders_with_unique.groupby('customer_unique_id').agg({\n",
        "    'order_id': 'count',\n",
        "    'customer_id': 'nunique'  # Quantos customer_id diferentes para cada unique_id\n",
        "}).rename(columns={'order_id': 'total_pedidos', 'customer_id': 'ids_diferentes'})\n",
        "\n",
        "print(f\"   Clientes únicos (por unique_id): {len(unique_customers):,}\")\n",
        "print(f\"   Distribuição por customer_unique_id:\")\n",
        "unique_recurrence = unique_customers['total_pedidos'].value_counts().sort_index()\n",
        "for pedidos, count in unique_recurrence.head(10).items():\n",
        "    pct = count / len(unique_customers) * 100\n",
        "    print(f\"     {pedidos} pedido(s): {count:,} clientes ({pct:.1f}%)\")\n",
        "\n",
        "# 4. Verificar se nossa limpeza afetou a recorrência\n",
        "print(f\"\\n4. IMPACTO DA LIMPEZA NA RECORRÊNCIA:\")\n",
        "clean_customers = orders_clean.groupby('customer_id')['order_id'].count()\n",
        "print(f\"   Clientes após limpeza: {len(clean_customers):,}\")\n",
        "print(f\"   Clientes com >1 pedido (após limpeza): {(clean_customers > 1).sum():,}\")\n",
        "\n",
        "# 5. Comparar customer_id que aparecem nos dados originais vs limpos\n",
        "original_customer_ids = set(orders['customer_id'])\n",
        "clean_customer_ids = set(orders_clean['customer_id'])\n",
        "lost_customers = len(original_customer_ids - clean_customer_ids)\n",
        "print(f\"   Customer_ids perdidos na limpeza: {lost_customers:,}\")\n",
        "\n",
        "print(f\"\\nCONCLUSÃO DA INVESTIGAÇÃO:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "q9eF_q2pi3Fq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3. CRIANDO FEATURES DE PRODUTOS E VENDEDORES...\n",
            "   Objetivo: Capturar qualidade, popularidade e performance\n",
            "   3.1 Calculando features de produtos...\n",
            "      Produtos únicos analisados: 32,214\n",
            "   3.2 Calculando features de vendedores...\n",
            "      Vendedores únicos analisados: 2,970\n",
            "   3.3 Mapeando features para os pedidos...\n",
            "      Pedidos com features de produto/vendedor: 96,470\n",
            "\n",
            "   ESTATISTICAS DE PRODUTOS E VENDEDORES:\n",
            "   Produtos populares (top 20%): 7,776\n",
            "   Vendedores alto volume (top 10%): 299\n",
            "   Pedidos com produto popular: 68,729\n",
            "   Pedidos com vendedor alto volume: 65,863\n",
            "\n",
            " RESUMO PARTE 3:\n",
            "   Product/Seller features: (96470, 7)\n",
            " Features de produtos e vendedores criadas!\n"
          ]
        }
      ],
      "source": [
        "# === PARTE 3: FEATURES DE PRODUTOS E VENDEDORES ===\n",
        "print(\"3. CRIANDO FEATURES DE PRODUTOS E VENDEDORES...\")\n",
        "print(\"   Objetivo: Capturar qualidade, popularidade e performance\")\n",
        "\n",
        "# 3.1 Features de produtos - popularidade e características\n",
        "print(\"   3.1 Calculando features de produtos...\")\n",
        "\n",
        "# Popularidade dos produtos (quantas vezes foram vendidos)\n",
        "product_popularity = items_clean.groupby('product_id').agg({\n",
        "    'order_id': 'nunique',                   # Quantos pedidos únicos\n",
        "    'order_item_id': 'count',                # Quantos itens vendidos total\n",
        "    'price': ['mean', 'std'],                # Preço médio e variação\n",
        "    'freight_value': 'mean'                  # Frete médio\n",
        "}).round(2)\n",
        "\n",
        "product_popularity.columns = [\n",
        "    'produto_qtd_pedidos', 'produto_qtd_vendas',\n",
        "    'produto_preco_medio', 'produto_std_preco', 'produto_frete_medio'\n",
        "]\n",
        "\n",
        "# Classificar produtos por popularidade\n",
        "product_popularity['produto_popular'] = (\n",
        "    product_popularity['produto_qtd_pedidos'] >=\n",
        "    product_popularity['produto_qtd_pedidos'].quantile(0.8)\n",
        ").astype(int)\n",
        "\n",
        "print(f\"      Produtos únicos analisados: {product_popularity.shape[0]:,}\")\n",
        "\n",
        "# 3.2 Features de vendedores - performance e volume\n",
        "print(\"   3.2 Calculando features de vendedores...\")\n",
        "\n",
        "# Performance dos vendedores\n",
        "seller_performance = items_clean.groupby('seller_id').agg({\n",
        "    'order_id': 'nunique',                   # Quantos pedidos únicos\n",
        "    'product_id': 'nunique',                 # Diversidade de produtos\n",
        "    'price': ['sum', 'mean'],                # Volume e ticket médio\n",
        "    'freight_value': 'mean'                  # Frete médio cobrado\n",
        "}).round(2)\n",
        "\n",
        "seller_performance.columns = [\n",
        "    'vendedor_qtd_pedidos', 'vendedor_diversidade_produtos',\n",
        "    'vendedor_volume_vendas', 'vendedor_ticket_medio', 'vendedor_frete_medio'\n",
        "]\n",
        "\n",
        "# Classificar vendedores por volume\n",
        "seller_performance['vendedor_alto_volume'] = (\n",
        "    seller_performance['vendedor_qtd_pedidos'] >=\n",
        "    seller_performance['vendedor_qtd_pedidos'].quantile(0.9)\n",
        ").astype(int)\n",
        "\n",
        "print(f\"      Vendedores únicos analisados: {seller_performance.shape[0]:,}\")\n",
        "\n",
        "# 3.3 Mapear features de volta para os pedidos\n",
        "print(\"   3.3 Mapeando features para os pedidos...\")\n",
        "\n",
        "# Para pedidos com múltiplos itens/vendedores, vamos agregar\n",
        "items_with_features = items_clean.merge(\n",
        "    product_popularity, left_on='product_id', right_index=True, how='left'\n",
        ").merge(\n",
        "    seller_performance, left_on='seller_id', right_index=True, how='left'\n",
        ")\n",
        "\n",
        "# Agregar por pedido (média para múltiplos itens)\n",
        "product_seller_features = items_with_features.groupby('order_id').agg({\n",
        "    'produto_qtd_pedidos': 'mean',\n",
        "    'produto_preco_medio': 'mean',\n",
        "    'produto_popular': 'max',                # Se pelo menos 1 produto popular\n",
        "    'vendedor_qtd_pedidos': 'mean',\n",
        "    'vendedor_ticket_medio': 'mean',\n",
        "    'vendedor_alto_volume': 'max',           # Se pelo menos 1 vendedor alto volume\n",
        "    'vendedor_diversidade_produtos': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "print(f\"      Pedidos com features de produto/vendedor: {product_seller_features.shape[0]:,}\")\n",
        "\n",
        "# 3.4 Estatísticas dos produtos e vendedores\n",
        "print(f\"\\n   ESTATISTICAS DE PRODUTOS E VENDEDORES:\")\n",
        "print(f\"   Produtos populares (top 20%): {product_popularity['produto_popular'].sum():,}\")\n",
        "print(f\"   Vendedores alto volume (top 10%): {seller_performance['vendedor_alto_volume'].sum():,}\")\n",
        "print(f\"   Pedidos com produto popular: {product_seller_features['produto_popular'].sum():,}\")\n",
        "print(f\"   Pedidos com vendedor alto volume: {product_seller_features['vendedor_alto_volume'].sum():,}\")\n",
        "\n",
        "print(f\"\\n RESUMO PARTE 3:\")\n",
        "print(f\"   Product/Seller features: {product_seller_features.shape}\")\n",
        "print(f\" Features de produtos e vendedores criadas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "grHoHUUtjEXW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4. CRIANDO FEATURES GEOGRÁFICAS E CONSOLIDANDO DATASET...\n",
            "   4.1 Criando features geográficas...\n",
            "      Features geográficas criadas: (96470, 11)\n",
            "   4.2 Consolidando todas as features em dataset único...\n",
            "      Adicionando features de itens...\n",
            "      Adicionando features de pagamentos...\n",
            "      Adicionando features de clientes...\n",
            "      Adicionando features de produtos/vendedores...\n",
            "      Adicionando features geográficas...\n",
            "\n",
            "   4.3 VERIFICAÇÃO DO DATASET CONSOLIDADO:\n",
            "      Shape final: (96470, 60)\n",
            "      Colunas criadas: 43\n",
            "      Valores nulos por coluna:\n",
            "        order_approved_at: 14 (0.0%)\n",
            "        order_delivered_carrier_date: 1 (0.0%)\n",
            "        qtd_formas_pagamento: 1 (0.0%)\n",
            "        total_parcelas: 1 (0.0%)\n",
            "        media_parcelas: 1 (0.0%)\n",
            "        max_parcelas: 1 (0.0%)\n",
            "        valor_total_pago: 1 (0.0%)\n",
            "        valor_medio_pagamento: 1 (0.0%)\n",
            "        std_valor_pagamento: 93,595 (97.0%)\n",
            "        usa_credit_card: 1 (0.0%)\n",
            "        usa_boleto: 1 (0.0%)\n",
            "        usa_debit_card: 1 (0.0%)\n",
            "        usa_voucher: 1 (0.0%)\n",
            "        diversidade_pagamento: 1 (0.0%)\n",
            "\n",
            " CONSOLIDAÇÃO CONCLUÍDA:\n",
            "   Dataset final: (96470, 60)\n",
            " Todas as features criadas e consolidadas com sucesso!\n",
            "\n",
            "   4.4 RESUMO DAS CATEGORIAS DE FEATURES:\n",
            "      Temporais: 2 features\n",
            "      Itens/Compra: 4 features\n",
            "      Pagamento: 4 features\n",
            "      Cliente: 2 features\n",
            "      Produto/Vendedor: 3 features\n",
            "      Geográficas: 3 features\n"
          ]
        }
      ],
      "source": [
        "# === PARTE 4: FEATURES GEOGRÁFICAS E CONSOLIDAÇÃO FINAL ===\n",
        "print(\"4. CRIANDO FEATURES GEOGRÁFICAS E CONSOLIDANDO DATASET...\")\n",
        "\n",
        "# 4.1 Features geográficas básicas\n",
        "print(\"   4.1 Criando features geográficas...\")\n",
        "\n",
        "# Mapear estados de clientes e vendedores para os pedidos\n",
        "customer_geo = customers[['customer_id', 'customer_state', 'customer_city']].rename(columns={\n",
        "    'customer_state': 'cliente_estado',\n",
        "    'customer_city': 'cliente_cidade'\n",
        "})\n",
        "\n",
        "# Para vendedores, vamos pegar o estado predominante por pedido\n",
        "seller_geo = items_clean.merge(\n",
        "    sellers[['seller_id', 'seller_state', 'seller_city']],\n",
        "    on='seller_id', how='left'\n",
        ").groupby('order_id').agg({\n",
        "    'seller_state': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0],  # Estado mais comum\n",
        "    'seller_city': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0]    # Cidade mais comum\n",
        "}).rename(columns={\n",
        "    'seller_state': 'vendedor_estado',\n",
        "    'seller_city': 'vendedor_cidade'\n",
        "})\n",
        "\n",
        "# Features geográficas derivadas\n",
        "geo_features = orders_clean[['order_id', 'customer_id']].merge(\n",
        "    customer_geo, on='customer_id', how='left'\n",
        ").merge(\n",
        "    seller_geo, left_on='order_id', right_index=True, how='left'\n",
        ").set_index('order_id')\n",
        "\n",
        "# Criar flag para entrega no mesmo estado\n",
        "geo_features['mesmo_estado'] = (geo_features['cliente_estado'] == geo_features['vendedor_estado']).astype(int)\n",
        "\n",
        "# Estados com maior volume (para criar features)\n",
        "top_customer_states = geo_features['cliente_estado'].value_counts().head(5).index\n",
        "top_seller_states = geo_features['vendedor_estado'].value_counts().head(5).index\n",
        "\n",
        "for state in top_customer_states:\n",
        "    geo_features[f'cliente_estado_{state}'] = (geo_features['cliente_estado'] == state).astype(int)\n",
        "\n",
        "print(f\"      Features geográficas criadas: {geo_features.shape}\")\n",
        "\n",
        "# 4.2 CONSOLIDAÇÃO FINAL - MERGE DE TODAS AS FEATURES\n",
        "print(\"   4.2 Consolidando todas as features em dataset único...\")\n",
        "\n",
        "# Começar com orders_clean como base\n",
        "dataset_final = orders_clean.set_index('order_id')\n",
        "\n",
        "# Adicionar todas as features criadas\n",
        "print(\"      Adicionando features de itens...\")\n",
        "dataset_final = dataset_final.join(items_features, how='left')\n",
        "\n",
        "print(\"      Adicionando features de pagamentos...\")\n",
        "dataset_final = dataset_final.join(payments_features, how='left')\n",
        "dataset_final = dataset_final.join(payment_types, how='left')\n",
        "\n",
        "print(\"      Adicionando features de clientes...\")\n",
        "dataset_final = dataset_final.join(customer_features, how='left')\n",
        "\n",
        "print(\"      Adicionando features de produtos/vendedores...\")\n",
        "dataset_final = dataset_final.join(product_seller_features, how='left')\n",
        "\n",
        "print(\"      Adicionando features geográficas...\")\n",
        "dataset_final = dataset_final.join(geo_features.drop(['customer_id'], axis=1), how='left')\n",
        "\n",
        "# 4.3 VERIFICAÇÃO FINAL DO DATASET\n",
        "print(f\"\\n   4.3 VERIFICAÇÃO DO DATASET CONSOLIDADO:\")\n",
        "print(f\"      Shape final: {dataset_final.shape}\")\n",
        "print(f\"      Colunas criadas: {dataset_final.shape[1] - 17}\")  # 17 = colunas originais + temporais\n",
        "print(f\"      Valores nulos por coluna:\")\n",
        "\n",
        "# Mostrar colunas com valores nulos\n",
        "null_counts = dataset_final.isnull().sum()\n",
        "null_cols = null_counts[null_counts > 0]\n",
        "if len(null_cols) > 0:\n",
        "    for col, count in null_cols.items():\n",
        "        pct = count / len(dataset_final) * 100\n",
        "        print(f\"        {col}: {count:,} ({pct:.1f}%)\")\n",
        "else:\n",
        "    print(\"        Nenhum valor nulo encontrado!\")\n",
        "\n",
        "print(f\"\\n CONSOLIDAÇÃO CONCLUÍDA:\")\n",
        "print(f\"   Dataset final: {dataset_final.shape}\")\n",
        "print(f\" Todas as features criadas e consolidadas com sucesso!\")\n",
        "\n",
        "# 4.4 Salvar resumo das features criadas\n",
        "print(f\"\\n   4.4 RESUMO DAS CATEGORIAS DE FEATURES:\")\n",
        "feature_categories = {\n",
        "    'Temporais': ['dias_para_entrega', 'dias_atraso', 'purchase_month', 'purchase_weekday', 'purchase_hour'],\n",
        "    'Itens/Compra': ['qtd_itens', 'valor_total_produtos', 'ticket_medio', 'frete_percentual'],\n",
        "    'Pagamento': ['qtd_formas_pagamento', 'media_parcelas', 'usa_credit_card', 'usa_boleto'],\n",
        "    'Cliente': ['total_pedidos_cliente', 'cliente_recorrente', 'is_primeiro_pedido'],\n",
        "    'Produto/Vendedor': ['produto_popular', 'vendedor_alto_volume', 'produto_preco_medio'],\n",
        "    'Geográficas': ['mesmo_estado', 'cliente_estado_SP', 'cliente_estado_RJ']\n",
        "}\n",
        "\n",
        "for category, features in feature_categories.items():\n",
        "    available = [f for f in features if f in dataset_final.columns]\n",
        "    print(f\"      {category}: {len(available)} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problema 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aprendizado Supervisionado (Classificação)\n",
        "**Objetivo**: Predizer a nota de avaliação (review_score) que um cliente dará a um pedido\n",
        "\n",
        "**Target**: review_score (variável categórica ordinal: 1, 2, 3, 4, 5)<br>\n",
        "**Tipo**: Classificação multi-classe com 5 categorias<br>\n",
        "**Desafio**: Distribuição desbalanceada (57.8% classe 5, 11.5% classe 1)\n",
        "\n",
        "\n",
        "**Justificativa**:\n",
        "- Permite à Olist antecipar satisfação do cliente\n",
        "- Identificar pedidos com risco de baixa avaliação\n",
        "- Otimizar estratégias de atendimento pré-entrega"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
